{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "10a76755-349a-4ff5-9778-261271245b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter company ticker (e.g. AAPL):  AAPL\n",
      "Enter filing year (e.g. 2024):  2020\n",
      "Enter keyword to match table text:  income\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found CIK 0000320193 for ticker AAPL\n",
      "https://data.sec.gov/submissions/CIK0000320193.json\n",
      "Found 10-K filing for AAPL 2020: https://www.sec.gov/Archives/edgar/data/320193/000032019320000096/aapl-20200926.htm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-04 22:55:48,915 INFO Found 23 matching tables for keyword 'income'\n",
      "2025-08-04 22:55:48,925 INFO Saved table 0 to sheet 'table_0'\n",
      "2025-08-04 22:55:48,931 INFO Saved table 1 to sheet 'table_1'\n",
      "2025-08-04 22:55:48,936 INFO Saved table 2 to sheet 'table_2'\n",
      "2025-08-04 22:55:48,940 INFO Saved table 3 to sheet 'table_3'\n",
      "2025-08-04 22:55:48,948 INFO Saved table 4 to sheet 'table_4'\n",
      "2025-08-04 22:55:48,953 INFO Saved table 5 to sheet 'table_5'\n",
      "2025-08-04 22:55:48,962 INFO Saved table 6 to sheet 'table_6'\n",
      "2025-08-04 22:55:48,970 INFO Saved table 7 to sheet 'table_7'\n",
      "2025-08-04 22:55:48,979 INFO Saved table 8 to sheet 'table_8'\n",
      "2025-08-04 22:55:48,984 INFO Saved table 9 to sheet 'table_9'\n",
      "2025-08-04 22:55:48,989 INFO Saved table 10 to sheet 'table_10'\n",
      "2025-08-04 22:55:48,994 INFO Saved table 11 to sheet 'table_11'\n",
      "2025-08-04 22:55:49,000 INFO Saved table 12 to sheet 'table_12'\n",
      "2025-08-04 22:55:49,004 INFO Saved table 13 to sheet 'table_13'\n",
      "2025-08-04 22:55:49,010 INFO Saved table 14 to sheet 'table_14'\n",
      "2025-08-04 22:55:49,017 INFO Saved table 15 to sheet 'table_15'\n",
      "2025-08-04 22:55:49,021 INFO Saved table 16 to sheet 'table_16'\n",
      "2025-08-04 22:55:49,027 INFO Saved table 17 to sheet 'table_17'\n",
      "2025-08-04 22:55:49,032 INFO Saved table 18 to sheet 'table_18'\n",
      "2025-08-04 22:55:49,037 INFO Saved table 19 to sheet 'table_19'\n",
      "2025-08-04 22:55:49,043 INFO Saved table 20 to sheet 'table_20'\n",
      "2025-08-04 22:55:49,048 INFO Saved table 22 to sheet 'table_22'\n",
      "2025-08-04 22:55:49,090 INFO All valid tables saved to tables_output/AAPL_2020_tables.xlsx\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup, XMLParsedAsHTMLWarning\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "# silence warnings when html parsed as xml\n",
    "warnings.filterwarnings(\"ignore\", category=XMLParsedAsHTMLWarning)\n",
    "\n",
    "#ignore junk tables i.e table of contents\n",
    "# usually if row is less than 2 cells in length then there is no data\n",
    "MIN_ROWS = 2\n",
    "# usually if column has less than 3 cells of length there is no data\n",
    "MIN_COLS = 3\n",
    "\n",
    "# build SEC url\n",
    "BASE = \"https://data.sec.gov/submissions/\"\n",
    "\n",
    "#reusable header for everywhere so website allows you to pass without seeming a bot\n",
    "HEADERS_URL = {\n",
    "    \"User-Agent\": \"MyResearchBot/1.0 (contact: myemail@example.com)\",\n",
    "    \"Accept-Encoding\": \"gzip, deflate\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "    \"Connection\": \"keep-alive\"}\n",
    "\n",
    "# map ticker to CIK\n",
    "TICKER_JSON = \"https://www.sec.gov/files/company_tickers.json\"\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s %(levelname)s %(message)s\",\n",
    "    level=logging.INFO\n",
    ")\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "#run script as ScrapingSECTablesHTMLImproved.py --ticker AAPL --year 2023 --keyword revenue\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Fetch and extract SEC 10-K tables into Excel\"\n",
    "    )\n",
    "    parser.add_argument(\"--ticker\", help=\"Company ticker (e.g. AAPL)\", default=None)\n",
    "    parser.add_argument(\"--year\", help=\"Filing year (e.g. 2024)\", default=None)\n",
    "    parser.add_argument(\"--keyword\", help=\"Keyword to match table text\", default=None)\n",
    "    parser.add_argument(\"--out-dir\", default=\"tables_output\", help=\"Output folder\")\n",
    "\n",
    "    # Accept unknown args\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    # Prompt user interactvely if missing arguments\n",
    "    if not args.ticker:\n",
    "        args.ticker = input(\"Enter company ticker (e.g. AAPL): \").strip()\n",
    "    if not args.year:\n",
    "        args.year = input(\"Enter filing year (e.g. 2024): \").strip()\n",
    "    if not args.keyword:\n",
    "        args.keyword = input(\"Enter keyword to match table text: \").strip()\n",
    "\n",
    "    return args\n",
    "\n",
    "# go from company ticker to CIK\n",
    "def get_cik_from_ticker(ticker: str) -> str:\n",
    "    url = TICKER_JSON\n",
    "    data = requests.get(url, headers=HEADERS_URL).json()\n",
    "    for entry in data.values():\n",
    "        # get json mapping from SEC and search of company ticker\n",
    "        if entry['ticker'].lower() == ticker.lower():\n",
    "            # fetch CIK\n",
    "            cik = str(entry['cik_str']).zfill(10)\n",
    "            print(f\"Found CIK {cik} for ticker {ticker}\")\n",
    "            return cik\n",
    "    raise ValueError(f\"Ticker {ticker} not found in SEC database\")\n",
    "\n",
    "# get SEC submisson JSON using CIK\n",
    "def get_10k_url(ticker: str, year: str) -> str:\n",
    "    cik = get_cik_from_ticker(ticker)\n",
    "    json_link = f\"{BASE}CIK{cik}.json\"\n",
    "    print(json_link)\n",
    "    headers = HEADERS_URL\n",
    "    resp = requests.get(json_link, headers=headers)\n",
    "    resp.raise_for_status()\n",
    "    if \"application/json\" not in resp.headers.get(\"Content-Type\", \"\"):\n",
    "        print(\"Unexpected content:\", resp.text[:200])\n",
    "        raise RuntimeError(f\"Did not receive JSON from SEC for {url}\")\n",
    "    data = resp.json()\n",
    "    forms = data['filings']['recent']\n",
    "    \n",
    "    filings = data.get(\"filings\", {}).get(\"recent\", {})\n",
    "    forms = filings.get(\"form\", [])\n",
    "    dates = filings.get(\"filingDate\", [])\n",
    "    accessions = filings.get(\"accessionNumber\", [])\n",
    "    documents = filings.get(\"primaryDocument\", [])\n",
    "\n",
    "    filings_data = list(zip(forms, dates, accessions, documents))\n",
    "    # Sort by date descending\n",
    "    filings_data.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # loop through filings and find first 10-K for year\n",
    "    for form, date, acc, doc in zip(forms, dates, accessions, documents):\n",
    "        #Check form type and date\n",
    "        if form.startswith(\"10-K\") and date.startswith(year): #and date == year:\n",
    "            # construct URL\n",
    "            acc_no_dashes = acc.replace(\"-\", \"\")\n",
    "            filing_url = (\n",
    "                f\"https://www.sec.gov/Archives/edgar/data/\"\n",
    "                f\"{int(cik)}/{acc_no_dashes}/{doc}\"\n",
    "            )\n",
    "            print(f\"Found 10-K filing for {ticker} {year}: {filing_url}\")\n",
    "            return filing_url  # return immediately\n",
    "\n",
    "    # if no 10-K found at all\n",
    "    raise ValueError(f\"No 10-K filing found for {ticker} in {year}\")\n",
    "\n",
    "# extract table and parse with Beautiful soup\n",
    "def extract_table(table) -> pd.DataFrame | None:\n",
    "    # parse table extracting row and cells\n",
    "    rows = []\n",
    "    for row in table.find_all(\"tr\"):\n",
    "        cells = []\n",
    "        for cell in row.find_all([\"td\", \"th\"]):\n",
    "            # normalize unicode\n",
    "            txt = unicodedata.normalize(\"NFKC\", cell.get_text(\" \", strip=True))\n",
    "            # trim trailing commas\n",
    "            if txt.endswith(\",\"): txt = txt.rstrip(\",\")\n",
    "            # fix non-breaking spaces\n",
    "            txt = txt.replace(u'\\xa0', ' ')\n",
    "            #skip lone % and $\n",
    "            if txt not in [\"$\", \"%\"]:\n",
    "                # convert negtaive values in paranthesis into numbers with \"-\" sign prefix\n",
    "                if txt.startswith(\"(\") and txt.endswith(\")\"):\n",
    "                    txt = \"-\" + txt[1:-1]\n",
    "                cells.append(txt)\n",
    "        if cells:\n",
    "            cleaned = []\n",
    "            for c in cells:\n",
    "                if not cleaned or c != cleaned[-1]:\n",
    "                    cleaned.append(c)\n",
    "            rows.append(cleaned)\n",
    "    # rows should be greater than 3 cells in length\n",
    "    if not rows or len(rows) < MIN_ROWS:\n",
    "        return None\n",
    "        \n",
    "    header = [c.strip().lower() for c in rows[0]]\n",
    "    first_col = [r[0].strip().lower() for r in rows]\n",
    "    # filtering tables with headers/first column with \"page\" or \"index\"\n",
    "    # these would be table of content data\n",
    "    if any(k in header + first_col for k in (\"page\", \"index\")):\n",
    "        return None\n",
    "    # padding row ton same length\n",
    "    max_cols = max(len(r) for r in rows)\n",
    "    rows = [r + ['']*(max_cols-len(r)) for r in rows]\n",
    "    df = pd.DataFrame(rows)\n",
    "    # reject tables with less than 3 columns after padding\n",
    "    if df.shape[1] < MIN_COLS:\n",
    "        return None\n",
    "    return df\n",
    "\n",
    "# main workflow\n",
    "def main():\n",
    "    # call parser\n",
    "    args = parse_args()\n",
    "    try:\n",
    "        url = get_10k_url(args.ticker, args.year)\n",
    "    except Exception as e:\n",
    "        log.error(f\"Failed to retrieve filing: {e}\")\n",
    "        return\n",
    "    \n",
    "    # get HTML doc\n",
    "    response = requests.get(url, headers=HEADERS_URL)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, \"lxml\")\n",
    "\n",
    "    # search table for key word given by user\n",
    "    all_tables = soup.find_all(\"table\")\n",
    "    target_tables = [\n",
    "        t for t in all_tables\n",
    "        if args.keyword.lower() in t.get_text(\" \", strip=True).lower()\n",
    "    ]\n",
    "\n",
    "    log.info(f\"Found {len(target_tables)} matching tables for keyword '{args.keyword}'\")\n",
    "    if not target_tables:\n",
    "        log.warning(\"No matching tables found.\")\n",
    "        return\n",
    "\n",
    "    output_dir = \"tables_output\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_file = os.path.join(output_dir, f\"{args.ticker}_{args.year}_tables.xlsx\")\n",
    "\n",
    "    # save valid tables into excel file (1 per sheet)\n",
    "    with pd.ExcelWriter(output_file, engine=\"openpyxl\") as writer:\n",
    "        for i, table_tag in enumerate(target_tables):\n",
    "            df = extract_table(table_tag)\n",
    "            if df is None:\n",
    "                continue\n",
    "            df = df.drop_duplicates().fillna(\"\")\n",
    "            df = df.loc[:, ~df.T.duplicated()]\n",
    "            sheet_name = f\"table_{i}\"[:31]\n",
    "            df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "            log.info(f\"Saved table {i} to sheet '{sheet_name}'\")\n",
    "\n",
    "    log.info(f\"All valid tables saved to {output_file}\")\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64697ac-a96f-472e-bbbf-36ea39e8b0a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
