{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6123cade-fae1-4e16-bef1-a5757f5b7b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "SEC GOV DATA ARCHIVES LINK eg https://www.sec.gov/Archives/edgar/data/1326801/000162828025036791 https://www.sec.gov/Archives/edgar/data/1018724/000101872425000086/0001018724-25-000086-index.htm\n",
      "EXTRACTED XBRL INSTANCE DOCUMENT eg meta-20250630_htm.xml amzn-20250630_htm.xml\n",
      "XBRL TAXONOMY EXTENSION CALCULATION LINKBASE DOCUMENT eg meta-20250630_cal.xml amzn-20250630_cal.xml\n",
      "XBRL TAXONOMY EXTENSION LABEL LINKBASE DOCUMENT eg meta-20250630_lab.xml amzn-20250630_lab.xml\n",
      "XBRL TAXONOMY EXTENSION DEFINITION LINKBASE DOCUMENT eg meta-20250630_def.xml amzn-20250630_def.xml\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded: XBRL_INSTANCE_DOCUMENT\n",
      "Downloaded: CALCULATION_LINKBASE\n",
      "Downloaded: LABEL_LINKBASE\n",
      "Downloaded: DEFINITION_LINKBASE\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import pprint\n",
    "import pathlib\n",
    "import collections\n",
    "# instead of beautiful soup parsing\n",
    "# API for parsing and creating XML data\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import lxml.etree as ETL\n",
    "\n",
    "import requests\n",
    "\n",
    "import argparse\n",
    "\n",
    "\n",
    "# build SEC url\n",
    "# BASE = \"https://data.sec.gov/submissions/\"\n",
    "# BASE = \"https://www.sec.gov/Archives/edgar/data/1326801/000162828025036791\"\n",
    "\n",
    "#reusable header for everywhere so website allows you to pass without seeming a bot\n",
    "HEADERS_URL = {\n",
    "    \"User-Agent\": \"MyResearchBot/1.0 (contact: myemail@example.com)\",\n",
    "    \"Accept-Encoding\": \"gzip, deflate\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "    \"Connection\": \"keep-alive\"}\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Fetch SEC XMLs\"\n",
    "    )\n",
    "    parser.add_argument(\"--url\", help=\"SEC GOV DATA ARCHIVES LINK eg https://www.sec.gov/Archives/edgar/data/1326801/000162828025036791\", default = None) #https://www.sec.gov/Archives/edgar/data/1326801/000162828025036791\n",
    "    parser.add_argument(\"--htm\", help=\"EXTRACTED XBRL INSTANCE DOCUMENT eg meta-20250630_htm.xml\", default=None) # meta-20250630_htm.xml\n",
    "    parser.add_argument(\"--cal\", help=\"XBRL TAXONOMY EXTENSION CALCULATION LINKBASE DOCUMENT\", default=None) # meta-20250630_cal.xml\n",
    "    parser.add_argument(\"--lab\", help=\"XBRL TAXONOMY EXTENSION LABEL LINKBASE DOCUMENT\", default=None) # meta-20250630_lab.xml\n",
    "    parser.add_argument(\"--defi\", help=\"XBRL TAXONOMY EXTENSION DEFINITION LINKBASE DOCUMENT\", default=None) # meta-20250630_def.xml\n",
    "\n",
    "    # Accept unknown args\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    # Prompt user interactvely if missing arguments\n",
    "    if not args.url:\n",
    "        args.url = input(\"SEC GOV DATA ARCHIVES LINK eg https://www.sec.gov/Archives/edgar/data/1326801/000162828025036791\").strip()\n",
    "        args.htm = input(\"EXTRACTED XBRL INSTANCE DOCUMENT eg meta-20250630_htm.xml\").strip()\n",
    "        args.cal = input(\"XBRL TAXONOMY EXTENSION CALCULATION LINKBASE DOCUMENT eg meta-20250630_cal.xml\").strip()\n",
    "        args.lab = input(\"XBRL TAXONOMY EXTENSION LABEL LINKBASE DOCUMENT eg meta-20250630_lab.xml\").strip()\n",
    "        args.defi = input(\"XBRL TAXONOMY EXTENSION DEFINITION LINKBASE DOCUMENT eg meta-20250630_def.xml\").strip()\n",
    "    return args\n",
    "\n",
    "\n",
    "# label categories\n",
    "avoid = ['linkbase', 'roleRef'] # labels without relevant info\n",
    "# labelArc points to next element you want\n",
    "parse = ['label', 'labeLink', 'labelArc', 'loc', 'definitionLink', 'definitionArc', 'calculationArc']\n",
    "\n",
    "# part of process is to create set of unique keys\n",
    "# set obj to house info\n",
    "# lookup benefit times vs use of list\n",
    "# plus sets ensur eonly unique values, no duplicates\n",
    "# create 2 sets to store keys\n",
    "\n",
    "#can loop through each file\n",
    "# P3\n",
    "def parse_linkbases(files_list, parse):\n",
    "\n",
    "    # list\n",
    "    storage_list = []\n",
    "    \n",
    "    # dictionary \n",
    "    storage_values= {}\n",
    "    \n",
    "    # another dictionary\n",
    "    storage_gaap = {}\n",
    "    \n",
    "    for file in files_list:\n",
    "    \n",
    "        #print(file)\n",
    "        # that returns first item in file tuple, which is a list\n",
    "        # we want to access file so we do files_list[0]\n",
    "        # i.e file_cal for instance\n",
    "    \n",
    "        # parse file\n",
    "        tree = ET.parse(file.file_path)\n",
    "        # create element tree\n",
    "        # print(tree)\n",
    "    \n",
    "        # grab all namespace_elements in tree\n",
    "        elements = tree.findall(file.namespace_element)\n",
    "        # will return all elements that match this: \n",
    "        # http://www.xbrl.org/2003/linkbase)calculationLink namespace\n",
    "        #print(elements)\n",
    "    \n",
    "        # loop through each element\n",
    "        # loop through child elements\n",
    "        # P4\n",
    "        for element in elements:\n",
    "            # create iterator\n",
    "            # loop through child element of each element\n",
    "            for child_element in element.iter():\n",
    "    \n",
    "                #print(child_element)\n",
    "                # get elements and their children from document\n",
    "                # next is getting attributes of elements\n",
    "                element_split_label = child_element.tag.split(\"}\")\n",
    "                # print(element_split_label)\n",
    "                # want to remove the redundant prefix on label:\n",
    "                # {http://www.xbrl.org/2003/linkbase}\n",
    "                # get parts of label\n",
    "                namespace = element_split_label[0]\n",
    "                label = element_split_label[1]\n",
    "                # is this label we want?\n",
    "                # wanted labels in parse\n",
    "                if label in parse:\n",
    "                    element_type_label = file.namespace_label + \"_\" + label\n",
    "                    #print(element_type_label)\n",
    "    \n",
    "                    # define dictionary\n",
    "                    dict_storage = {}\n",
    "                    dict_storage[\"item_type\"] = element_type_label\n",
    "    \n",
    "                    # get attribute keys\n",
    "                    cal_keys = child_element.keys()\n",
    "                    # print(cal_keys)\n",
    "    \n",
    "                    for key in cal_keys:\n",
    "                        if \"}\" in key:\n",
    "                            new_key = key.split(\"}\")[1]\n",
    "                            dict_storage[new_key] = child_element.attrib[key]\n",
    "                        else:\n",
    "                            dict_storage[key] = child_element.attrib[key]\n",
    "                    #print(dict_storage)\n",
    "    \n",
    "                    # choosing master key to be the label document\n",
    "                    # could choose anything else - experimental\n",
    "                    if element_type_label == \"label_label\":\n",
    "                        key_store = dict_storage[\"label\"]\n",
    "    \n",
    "                        # create master key\n",
    "                        master_key = key_store.replace(\"lab_\", \"\")\n",
    "    \n",
    "                        # split master key\n",
    "                        label_split = master_key.split(\"_\")\n",
    "    \n",
    "                        #a\n",
    "                        # create gaap id\n",
    "                        gaap_id =  label_split[0] + \";\" + label_split[1]\n",
    "                        #print(label_split)\n",
    "                        # there are duplicates\n",
    "                        # thats why we put it in a dicionary - unique key to value\n",
    "                        # dict for xml files\n",
    "                        storage_values[master_key] = {}\n",
    "                        # dictionary storage values is created with the master key\n",
    "                      \n",
    "                        storage_values[master_key][\"label_id\"] = key_store\n",
    "                        storage_values[master_key][\"location_id\"] = key_store.replace(\"lab_\", \"loc_\")\n",
    "                        storage_values[master_key][\"us_gaap_id\"] = gaap_id\n",
    "                        storage_values[master_key][\"us_gaap_values\"] = None\n",
    "                        storage_values[master_key][element_type_label] = dict_storage\n",
    "                        #b is a subdictiory of a\n",
    "                        # dict for only values related to GAAP\n",
    "                        storage_gaap[gaap_id] = {}\n",
    "                        storage_gaap[gaap_id][\"id\"] = gaap_id\n",
    "                        storage_gaap[gaap_id][\"master_id\"] = master_key\n",
    "                        # a and b should be merged\n",
    "                        # master keys created in big dictiory\n",
    "                        # master key associated with smaller dictiory for GAAP stuff exclusively, organized as in b\n",
    "                        # add to dict\n",
    "                    storage_list.append([file.namespace_label, dict_storage])\n",
    "                    # parsing the html file with nonNumeric and nonFractional stuff\n",
    "                    # parse 10Q file\n",
    "                    # load file_htm\n",
    "    # del if breaks\n",
    "    return storage_list, storage_values, storage_gaap\n",
    "\n",
    "\n",
    "def parse_instance_doc(file_htm, storage_values, storage_list, storage_gaap):        \n",
    "    tree = ET.parse(file_htm)\n",
    "    # Process nonNumeric elements\n",
    "    for element in tree.iter():\n",
    "        #print(element.attrib)\n",
    "            if \"nonNumeric\" in element.tag or \"nonFractional\" in element.tag:\n",
    "                # get attribute name and master id\n",
    "                attr_name = element.attrib[\"name\"]\n",
    "                gaap_id = storage_gaap[attr_name][\"master_id\"]\n",
    "            \n",
    "                storage_gaap[attr_name][\"context_ref\"] = element.attrib[\"contextRef\"]\n",
    "                storage_gaap[attr_name][\"context_id\"] = element.attrib[\"id\"]\n",
    "                storage_gaap[attr_name][\"continued_at\"] = element.attrib.get(\"continuedAt\", \"null\")\n",
    "                storage_gaap[attr_name][\"escape\"] = element.attrib.get(\"escape\", \"null\")\n",
    "                storage_gaap[attr_name][\"format\"] = element.attrib.get(\"format\", \"null\")\n",
    "                storage_gaap[attr_name][\"unit_ref\"] = element.attrib.get(\"unitRef\", \"null\")\n",
    "                storage_gaap[attr_name][\"decimals\"] = element.attrib.get(\"decimals\", \"null\")\n",
    "                storage_gaap[attr_name][\"scale\"] = element.attrib.get(\"scale\", \"null\")\n",
    "                storage_gaap[attr_name][\"format\"] = element.attrib.get(\"format\", \"null\")\n",
    "                storage_gaap[attr_name][\"value\"] = element.text.strip() if element.text else \"null\"\n",
    "        \n",
    "                if gaap_id in storage_values:\n",
    "                    storage_values[gaap_id][\"us_gaap_value\"] = storage_gaap[attr_name]\n",
    "\n",
    "\n",
    "def write_csv(storage_list, storage_values):\n",
    "    # create csv\n",
    "    file_name = \"sec_xbrl_scrape_content.csv\"\n",
    "\n",
    "    with open(file_name, mode = \"w\", newline = \"\") as sec_file:\n",
    "        #create writer\n",
    "        writer = csv.writer(sec_file)\n",
    "        # write the header\n",
    "        # pass to the row writer the list of things to go into the header\n",
    "        writer.writerow([\"FILE\", \"LABEL\", \"VALUE\"])\n",
    "        # dump dict into csv\n",
    "        for dict_cont in storage_list:\n",
    "            # write row by row the things stored inside the storage list\n",
    "            # the first is the namespace label\n",
    "            # the second item is the actual dict\n",
    "            for item in dict_cont[1].items():\n",
    "                # second item is list of lists\n",
    "                # grab items per each item\n",
    "                writer.writerow([dict_cont[0]] + list(item))\n",
    "            \n",
    "    # create csv\n",
    "    file_name = \"sec_xbrl_scrape_values.csv\"\n",
    "\n",
    "    with open(file_name, mode = \"w\", newline = \"\") as sec_file:\n",
    "        writer = csv.writer(sec_file)\n",
    "        writer.writerow([\"ID\", \"CATEGORY\", \"LABEL\", \"VALUE\"])\n",
    "        for storage1 in storage_values:\n",
    "            # storage1 are keys to the values extracted from the second level dict\n",
    "            # the .items() call enumerates values in dict\n",
    "            for storage2 in storage_values[storage1].items():\n",
    "                # extract by key the value\n",
    "                # the value might be another dict because elements can have child elements\n",
    "                if isinstance(storage2[1], dict): # check if it is\n",
    "                    for storage3 in storage2[1].items():\n",
    "                        # write to csv\n",
    "                        writer.writerow([storage1] + [storage2[0]] + list(storage3))\n",
    "                else:\n",
    "                    if storage2[1] != None:\n",
    "                        #write to csv, if storage2 is not a dictionry (we dont go to storage3)\n",
    "                        writer.writerow([storage1] + [storage2] + [\"None\"])\n",
    "# main workflow\n",
    "def main():\n",
    "\n",
    "    # call parser\n",
    "    args = parse_args()\n",
    "\n",
    "    # list\n",
    "    storage_list = []\n",
    "\n",
    "    # dictionary \n",
    "    storage_values= {}\n",
    "    \n",
    "    # another dictionary\n",
    "    storage_gaap = {}\n",
    "\n",
    "    \n",
    "    fmap = {\n",
    "    \"XBRL_INSTANCE_DOCUMENT\" : args.htm,  #\"meta-20250630_htm.xml\": \"meta-20250630_htm.xml\",\n",
    "    \"CALCULATION_LINKBASE\" : args.cal,  # \"meta-20250630_cal.xml\": \"meta-20250630_cal.xml\",\n",
    "    \"LABEL_LINKBASE\" : args.lab,  #\"meta-20250630_lab.xml\": \"meta-20250630_lab.xml\",\n",
    "    \"DEFINITION_LINKBASE\" : args.defi # \"meta-20250630_def.xml\": \"meta-20250630_def.xml\",\n",
    "    }\n",
    "\n",
    "    # define working directory\n",
    "    # obj stores cur directory\n",
    "    # joining with a folder containing downloaded documents\n",
    "    # later should replace with scraping\n",
    "    # assumes folder exists, builds path to that folder\n",
    "    sec_directory = pathlib.Path.cwd().joinpath(\"folder_to_store_xml_docs\")\n",
    "    sec_directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    url_base = args.url\n",
    "    if url_base.endswith(\"-index.htm\"):\n",
    "        url_base = url_base.rsplit(\"/\", 1)[0]  # keep only the folder path\n",
    "    \n",
    "    for fname, tail in fmap.items():\n",
    "        fpath = sec_directory / tail\n",
    "        if not fpath.exists():\n",
    "            try:\n",
    "                response = requests.get(f\"{url_base}/{tail}\", headers = HEADERS_URL)\n",
    "                response.raise_for_status()\n",
    "                fpath.write_bytes(response.content)\n",
    "                print(f\"Downloaded: {fname}\")\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"Error downloading {tail}: {e}\")\n",
    "        else:\n",
    "            print(f\"Already exists: {fname}\")\n",
    "\n",
    "    # define file paths to documents\n",
    "    # taken straight from data files\n",
    "    # in this case for meta\n",
    "    # https://www.sec.gov/Archives/edgar/data/1326801/000162828025036791/0001628280-25-036791-index.htm\n",
    "    # the code just builds a file path to the files in the folder, assumes they are already there\n",
    "    # so they should b placed there already for this to work\n",
    "    file_htm = sec_directory.joinpath(args.htm).resolve() # htm\n",
    "    file_cal = sec_directory.joinpath(args.cal).resolve() # calculation\n",
    "    file_lab = sec_directory.joinpath(args.lab).resolve() # label\n",
    "    file_def = sec_directory.joinpath(args.defi).resolve() # definition\n",
    "\n",
    "    # create constructor for named tuple object type\n",
    "    FilingTuple = collections.namedtuple(\"FilingTuple\", [\"file_path\", \"namespace_element\", \"namespace_label\"])\n",
    "    # create 3 of those each with a doc that will be parsed\n",
    "    # file path already defined in P1, name space element, name space label\n",
    "    # link to an online thing\n",
    "    \n",
    "    files_list = [\n",
    "        FilingTuple(file_cal, '{http://www.xbrl.org/2003/linkbase}calculationLink', 'calculation'),\n",
    "        FilingTuple(file_def, '{http://www.xbrl.org/2003/linkbase}definitionLink', 'definition'),\n",
    "        FilingTuple(file_lab, '{http://www.xbrl.org/2003/linkbase}labelLink', 'label')\n",
    "    ]\n",
    "    \n",
    "    storage_list, storage_values, storage_gaap = parse_linkbases(files_list, parse)\n",
    "    parse_instance_doc(file_htm, storage_values, storage_list, storage_gaap)\n",
    "    write_csv(storage_list, storage_values)\n",
    "    \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7217730-aafd-47f9-954b-b6f1f2a2d666",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
