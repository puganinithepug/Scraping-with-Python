{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9873bf9d-cca1-47da-8822-f15a5fbb4894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Company ticker amzn\n",
      "Date 2020\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found CIK 0001018724 for ticker amzn\n",
      "https://data.sec.gov/submissions/CIK0001018724.json\n",
      "https://www.sec.gov/Archives/edgar/data/1018724/000101872425000090/index.json\n",
      "https://www.sec.gov/Archives/edgar/data/1018724/000101872425000088/index.json\n",
      "https://www.sec.gov/Archives/edgar/data/1018724/000110465925074202/index.json\n",
      "https://www.sec.gov/Archives/edgar/data/1018724/000101872425000086/index.json\n",
      "Found filing with required XMLs at: https://www.sec.gov/Archives/edgar/data/1018724/000101872425000086/index.json\n",
      "{'amzn-20250630_cal.xml': 'https://www.sec.gov/Archives/edgar/data/1018724/000101872425000086/amzn-20250630_cal.xml', 'amzn-20250630_def.xml': 'https://www.sec.gov/Archives/edgar/data/1018724/000101872425000086/amzn-20250630_def.xml', 'amzn-20250630_htm.xml': 'https://www.sec.gov/Archives/edgar/data/1018724/000101872425000086/amzn-20250630_htm.xml', 'amzn-20250630_lab.xml': 'https://www.sec.gov/Archives/edgar/data/1018724/000101872425000086/amzn-20250630_lab.xml', 'amzn-20250630_pre.xml': 'https://www.sec.gov/Archives/edgar/data/1018724/000101872425000086/amzn-20250630_pre.xml'}\n",
      "Already exists: amzn-20250630_cal.xml\n",
      "Already exists: amzn-20250630_def.xml\n",
      "Already exists: amzn-20250630_htm.xml\n",
      "Already exists: amzn-20250630_lab.xml\n",
      "Downloaded: amzn-20250630_pre.xml\n",
      "Written to csv successfully\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import pprint\n",
    "import pathlib\n",
    "import collections\n",
    "# API for parsing and creating XML data\n",
    "import xml.etree.ElementTree as ET\n",
    "import lxml.etree as ETL\n",
    "import requests\n",
    "import argparse\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import math\n",
    "import decimal as Decimal\n",
    "import re\n",
    "\n",
    "# map ticker to CIK\n",
    "TICKER_JSON = \"https://www.sec.gov/files/company_tickers.json\"\n",
    "\n",
    "# get accession number from cik\n",
    "SUB_URL = \"https://data.sec.gov/submissions/\"\n",
    "\n",
    "BASE = \"https://www.sec.gov/Archives/edgar/data/\"\n",
    "\n",
    "#reusable header for sec.gov, complying with standards so website allows you to pass without seeming a bot\n",
    "HEADERS_URL = {\n",
    "    \"User-Agent\": \"MyResearchBot/1.0 (contact: myemail@example.com)\",\n",
    "    \"Accept-Encoding\": \"gzip, deflate\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "    \"Connection\": \"keep-alive\"}\n",
    "\n",
    "# keys used to parse the document for desired data\n",
    "parse = ['label', 'labelLink', 'labelArc', 'loc', 'definitionLink', 'definitionArc', 'calculationArc', 'presentationLink', 'presentationArc', 'presentation']\n",
    "\n",
    "# Max CIK matches count\n",
    "MAX_COUNT = 100\n",
    "\n",
    "# parser\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Fetch SEC XMLs\"\n",
    "    )\n",
    "    parser.add_argument(\"--ticker\", help=\"Company ticker\", default=None)\n",
    "    parser.add_argument(\"--date\", help=\"Date\", default=None)\n",
    "    # Accept unknown args\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    # user interactvely if missing arguments\n",
    "    if not args.ticker:\n",
    "        args.ticker = input(\"Company ticker\")\n",
    "        args.date = input(\"Date\")\n",
    "    return args\n",
    "\n",
    "def parse_numeric_text_to_float(text, decimals=None):\n",
    "    \"\"\"\n",
    "    Convert XBRL text to a float. Handles commas, parentheses for negatives,\n",
    "    scientific notation, and attempts a safe Decimal -> float conversion.\n",
    "    Applies a simple interpretation of 'decimals' if provided.\n",
    "    Returns float or None.\n",
    "    \"\"\"\n",
    "    if text is None:\n",
    "        return None\n",
    "\n",
    "    s = str(text).strip()\n",
    "    if s == \"\" or s.lower() in (\"null\", \"n/a\"):\n",
    "        return None\n",
    "\n",
    "    # Remove thousands separators\n",
    "    s = s.replace(\",\", \"\").strip()\n",
    "\n",
    "    negative = False\n",
    "    if s.startswith(\"(\") and s.endswith(\")\"):\n",
    "        negative = True\n",
    "        s = s[1:-1].strip()\n",
    "\n",
    "    # Extract a numeric substring (handles e/E scientific notation)\n",
    "    m = re.search(r\"[-+]?\\d+(\\.\\d+)?([eE][-+]?\\d+)?\", s)\n",
    "    if not m:\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        val = Decimal(m.group(0))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "    # XBRL uses decimals=-3 indicates thousands\n",
    "    if decimals is not None:\n",
    "        try:\n",
    "            d = int(decimals)\n",
    "            # scaled = val * (10 ** (-d))\n",
    "            val = val * (Decimal(10) ** (-d))\n",
    "        except Exception:\n",
    "            # ignore malformed\n",
    "            pass\n",
    "\n",
    "    if negative:\n",
    "        val = -val\n",
    "\n",
    "    try:\n",
    "        f = float(val)\n",
    "        if math.isfinite(f):\n",
    "            return f\n",
    "        return None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def extract_root_nsmap_and_prefixes(file_path):\n",
    "    try:\n",
    "        root = ETL.parse(str(file_path)).getroot()\n",
    "        nsmap = root.nsmap or {}\n",
    "        uri_to_prefix = {}\n",
    "        for prefix, uri in nsmap.items():\n",
    "            if uri is None:\n",
    "                continue\n",
    "            if prefix is None:\n",
    "                uri_to_prefix[uri] = \"\"  # default namespace => empty prefix\n",
    "            else:\n",
    "                uri_to_prefix[uri] = prefix\n",
    "        return uri_to_prefix\n",
    "    except Exception:\n",
    "        return {}\n",
    " \n",
    "\n",
    "# go from company ticker to CIK\n",
    "def get_cik_from_ticker(ticker: str) -> str:\n",
    "    url = TICKER_JSON\n",
    "    data = requests.get(url, headers=HEADERS_URL).json()\n",
    "    for entry in data.values():\n",
    "        # get json mapping from SEC and search of company ticker\n",
    "        if entry['ticker'].lower() == ticker.lower():\n",
    "            # fetch CIK\n",
    "            cik = str(entry['cik_str']).zfill(10)\n",
    "            print(f\"Found CIK {cik} for ticker {ticker}\")\n",
    "            return cik\n",
    "    raise ValueError(f\"Ticker {ticker} not found in SEC database\")\n",
    "\n",
    "# creates a mapping for xml documents and their urls\n",
    "def get_url(cik, ticker, year: str) -> str:\n",
    "    # cik = get_cik_from_ticker(ticker)\n",
    "    json_link = f\"{SUB_URL}CIK{cik}.json\"\n",
    "    print(json_link)\n",
    "    headers = HEADERS_URL\n",
    "    resp = requests.get(json_link, headers=headers)\n",
    "    resp.raise_for_status()\n",
    "    if \"application/json\" not in resp.headers.get(\"Content-Type\", \"\"):\n",
    "        print(\"Unexpected content:\", resp.text[:200])\n",
    "        raise RuntimeError(f\"Did not receive JSON from SEC for {url}\")\n",
    "    data = resp.json()\n",
    "    forms = data['filings']['recent']\n",
    "    \n",
    "    filings = data.get(\"filings\", {}).get(\"recent\", {})\n",
    "    forms = filings.get(\"form\", [])\n",
    "    dates = filings.get(\"filingDate\", [])\n",
    "    accessions = filings.get(\"accessionNumber\", [])\n",
    "    documents = filings.get(\"primaryDocument\", [])\n",
    "\n",
    "    filings_data = list(zip(forms, dates, accessions, documents))\n",
    "    # by date descending\n",
    "    filings_data.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    for form, date, acc, doc in filings_data:\n",
    "        acc_no_dashes = acc.replace(\"-\", \"\")\n",
    "        # base URL\n",
    "        base_url = f\"{BASE}{int(cik)}/{acc_no_dashes}/\"\n",
    "        \n",
    "        # filing's index.json (listing all files)\n",
    "        index_url = f\"{base_url}index.json\"\n",
    "        print(index_url)\n",
    "        try:\n",
    "            r = requests.get(index_url, headers=headers)\n",
    "            r.raise_for_status()\n",
    "            idx_data = r.json()\n",
    "\n",
    "            # relevant XML files found here\n",
    "            found_files = {}\n",
    "            for file_entry in idx_data.get(\"directory\", {}).get(\"item\", []):\n",
    "                name = file_entry.get(\"name\", \"\")\n",
    "                # check if file is one of the XML types you want\n",
    "                if any(suffix in name for suffix in [\"_def.xml\", \"_htm.xml\", \"_lab.xml\", \"_cal.xml\", \"_pre.xml\"]):\n",
    "                    # keys without extensions for clarity or keep names as is\n",
    "                    found_files[name] = base_url + name\n",
    "\n",
    "            # found at least the instance document (_htm.xml), consider success\n",
    "            if any(\"_htm.xml\" in fname for fname in found_files):\n",
    "                print(f\"Found filing with required XMLs at: {index_url}\")\n",
    "                print(found_files)\n",
    "                # scans directory items for the XML files you want (_def.xml, _htm.xml, _lab.xml, _cal.xml\n",
    "                # returns dict with found required elements of fmap (some may be missing)\n",
    "                return found_files  # return dictionary of files with full URLs\n",
    "\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Error fetching {index_url}: {e}\")\n",
    "            continue  # Try next filing\n",
    "\n",
    "    # If none found\n",
    "    raise ValueError(f\"No filing found with required XML files for CIK {cik} and year {year}\")\n",
    "\n",
    "#can loop through each file\n",
    "# P3\n",
    "# del if breaks\n",
    "def parse_linkbases(files_list, parse_labels):\n",
    "    storage_list = []\n",
    "    storage_values = {}\n",
    "    storage_gaap = {}\n",
    "\n",
    "    for file in files_list:\n",
    "        # expect FilingTuple(file_path, namespace_element, namespace_label)\n",
    "        if not getattr(file, \"file_path\", None):\n",
    "            print(f\"Warning: File for {getattr(file, 'namespace_label', '?')} not found, skipping.\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            tree = ET.parse(str(file.file_path))\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing {file.file_path}: {e}\")\n",
    "            continue\n",
    "\n",
    "        root = tree.getroot()\n",
    "        elements = root.findall(file.namespace_element)\n",
    "        for element in elements:\n",
    "            for child_element in element.iter():\n",
    "                tag = child_element.tag if isinstance(child_element.tag, str) else \"\"\n",
    "                # safe localname\n",
    "                local = tag.split(\"}\")[-1] if \"}\" in tag else tag\n",
    "                if local in parse_labels:\n",
    "                    element_type_label = f\"{file.namespace_label}_{local}\"\n",
    "\n",
    "                    dict_storage = {\"item_type\": element_type_label}\n",
    "                    # transfer attributes; strip namespace from attrib keys that contain '}'\n",
    "                    for key, val in child_element.attrib.items():\n",
    "                        if \"}\" in key:\n",
    "                            new_key = key.split(\"}\", 1)[1]\n",
    "                        else:\n",
    "                            new_key = key\n",
    "                        dict_storage[new_key] = val\n",
    "\n",
    "                    # handle label-specific mapping (your original logic)\n",
    "                    if element_type_label == \"label_label\" and \"label\" in dict_storage:\n",
    "                        key_store = dict_storage[\"label\"]\n",
    "                        master_key = key_store.replace(\"lab_\", \"\")\n",
    "                        label_split = master_key.split(\"_\")\n",
    "                        if len(label_split) >= 2:\n",
    "                            gaap_id = f\"{label_split[0]};{label_split[1]}\"\n",
    "                        else:\n",
    "                            gaap_id = master_key\n",
    "\n",
    "                        storage_values.setdefault(master_key, {})\n",
    "                        storage_values[master_key].update({\n",
    "                            \"label_id\": key_store,\n",
    "                            \"location_id\": key_store.replace(\"lab_\", \"loc_\"),\n",
    "                            \"us_gaap_id\": gaap_id,\n",
    "                            \"us_gaap_values\": None,\n",
    "                        })\n",
    "                        storage_values[master_key][element_type_label] = dict_storage\n",
    "\n",
    "                        storage_gaap.setdefault(gaap_id, {})\n",
    "                        storage_gaap[gaap_id].update({\n",
    "                            \"id\": gaap_id,\n",
    "                            \"master_id\": master_key\n",
    "                        })\n",
    "                    else:\n",
    "                        # store other linkbase entries keyed by their item label\n",
    "                        # make unique key\n",
    "                        storage_list.append([file.namespace_label, dict_storage])\n",
    "\n",
    "    return storage_list, storage_values, storage_gaap\n",
    "\n",
    "\n",
    "def parse_instance_doc(file_htm, storage_values, storage_list, storage_gaap):\n",
    "    storage_facts = []\n",
    "    uri_to_prefix = extract_root_nsmap_and_prefixes(file_htm)\n",
    "\n",
    "    try:\n",
    "        tree = ET.parse(str(file_htm))\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing instance document {file_htm}: {e}\")\n",
    "        return\n",
    "\n",
    "    root = tree.getroot()\n",
    "\n",
    "    #context and unit maps\n",
    "    contexts = {}\n",
    "    units = {}\n",
    "    for el in root.iter():\n",
    "        tag = el.tag if isinstance(el.tag, str) else \"\"\n",
    "        local = tag.split(\"}\")[-1] if \"}\" in tag else tag\n",
    "        if local == \"context\":\n",
    "            cid = el.attrib.get(\"id\")\n",
    "            if cid:\n",
    "                # entity.identifier and period\n",
    "                entity_id = None\n",
    "                period_start = None\n",
    "                period_end = None\n",
    "                instant = None\n",
    "                for sub in el:\n",
    "                    sub_local = sub.tag.split(\"}\")[-1] if \"}\" in sub.tag else sub.tag\n",
    "                    if sub_local == \"entity\":\n",
    "                        id_el = sub.find(\".//{*}identifier\")\n",
    "                        if id_el is not None:\n",
    "                            entity_id = id_el.text\n",
    "                    elif sub_local == \"period\":\n",
    "                        start_el = sub.find(\".//{*}startDate\")\n",
    "                        end_el = sub.find(\".//{*}endDate\")\n",
    "                        instant_el = sub.find(\".//{*}instant\")\n",
    "                        if start_el is not None:\n",
    "                            period_start = start_el.text\n",
    "                        if end_el is not None:\n",
    "                            period_end = end_el.text\n",
    "                        if instant_el is not None:\n",
    "                            instant = instant_el.text\n",
    "                contexts[cid] = {\n",
    "                    \"entity_identifier\": entity_id,\n",
    "                    \"period_start\": period_start,\n",
    "                    \"period_end\": period_end,\n",
    "                    \"instant\": instant\n",
    "                }\n",
    "        elif local == \"unit\":\n",
    "            uid = el.attrib.get(\"id\")\n",
    "            if uid:\n",
    "                # store unit element text or measures found\n",
    "                measures = [m.text for m in el.findall(\".//{*}measure\")]\n",
    "                units[uid] = {\"measures\": measures}\n",
    "\n",
    "    # attach contexts/units to storage_values for CSV output\n",
    "    storage_values[\"_contexts\"] = contexts\n",
    "    storage_values[\"_units\"] = units\n",
    "\n",
    "    # iterate facts\n",
    "    for element in root.iter():\n",
    "        tag = element.tag if isinstance(element.tag, str) else \"\"\n",
    "        local = tag.split(\"}\")[-1] if \"}\" in tag else tag\n",
    "\n",
    "        #nonNumeric/nonFractional specially\n",
    "        if \"nonNumeric\" in tag or \"nonFractional\" in tag:\n",
    "            attr_name = element.attrib.get(\"name\")\n",
    "            if not attr_name:\n",
    "                continue\n",
    "            if attr_name in storage_gaap:\n",
    "                g = storage_gaap[attr_name]\n",
    "                g[\"context_ref\"] = element.attrib.get(\"contextRef\")\n",
    "                g[\"context_id\"] = element.attrib.get(\"id\")\n",
    "                g[\"continued_at\"] = element.attrib.get(\"continuedAt\", \"null\")\n",
    "                g[\"escape\"] = element.attrib.get(\"escape\", \"null\")\n",
    "                g[\"format\"] = element.attrib.get(\"format\", \"null\")\n",
    "                g[\"unit_ref\"] = element.attrib.get(\"unitRef\", \"null\")\n",
    "                g[\"decimals\"] = element.attrib.get(\"decimals\", \"null\")\n",
    "                g[\"scale\"] = element.attrib.get(\"scale\", \"null\")\n",
    "                g[\"value\"] = element.text.strip() if element.text else \"null\"\n",
    "\n",
    "                gaap_key = attr_name\n",
    "                if gaap_key in storage_gaap:\n",
    "                    master = storage_gaap[gaap_key].get(\"master_id\")\n",
    "                    if master:\n",
    "                        storage_values.setdefault(master, {})[\"us_gaap_value\"] = storage_gaap[attr_name]\n",
    "                else:\n",
    "                    storage_values.setdefault(\"_unmapped_nonNumeric\", {})[attr_name] = storage_gaap[attr_name]\n",
    "            else:\n",
    "                # store unmapped nonNumeric if desired\n",
    "                storage_values.setdefault(\"_unmapped_nonNumeric\", {})[attr_name] = {\n",
    "                    \"context_ref\": element.attrib.get(\"contextRef\"),\n",
    "                    \"value\": element.text.strip() if element.text else None\n",
    "                }\n",
    "            # continue to next element (nonNumeric handled)\n",
    "            continue\n",
    "\n",
    "        # skip things that are not facts (no contextRef)\n",
    "        ctx = element.attrib.get(\"contextRef\")\n",
    "        if ctx is None:\n",
    "            continue\n",
    "\n",
    "        # build gaap_candidate using namespace mapping\n",
    "        if \"}\" in tag:\n",
    "            ns_uri = tag.split(\"}\")[0].lstrip(\"{\")\n",
    "            localname = tag.split(\"}\")[1]\n",
    "        else:\n",
    "            ns_uri = None\n",
    "            localname = tag\n",
    "\n",
    "        prefix = uri_to_prefix.get(ns_uri) if ns_uri else None\n",
    "        if prefix in (None, \"\"):\n",
    "            gaap_candidate = localname\n",
    "        else:\n",
    "            gaap_candidate = f\"{prefix};{localname}\"\n",
    "\n",
    "        unit_ref = element.attrib.get(\"unitRef\")\n",
    "        decimals = element.attrib.get(\"decimals\")\n",
    "\n",
    "        \n",
    "        value_raw = element.text.strip() if element.text else None\n",
    "        value_numeric = parse_numeric_text_to_float(value_raw, decimals=decimals)\n",
    "        \n",
    "\n",
    "        fact = {\n",
    "            \"tag_local\": localname,\n",
    "            \"tag_prefix\": prefix,\n",
    "            \"gaap_id_candidate\": gaap_candidate,\n",
    "            \"contextRef\": ctx,\n",
    "            \"unitRef\": unit_ref,\n",
    "            \"decimals\": decimals,\n",
    "            \"value_raw\": value_raw,\n",
    "            \"value_numeric\": value_numeric\n",
    "        }\n",
    "        storage_facts.append(fact)\n",
    "\n",
    "        if gaap_candidate in storage_gaap:\n",
    "            master_key = storage_gaap[gaap_candidate][\"master_id\"]\n",
    "            storage_values.setdefault(master_key, {}).setdefault(\"facts\", []).append(fact)\n",
    "        else:\n",
    "            storage_values.setdefault(\"_unmapped_facts\", []).append(fact)\n",
    "\n",
    "    storage_values[\"_facts_list\"] = storage_facts\n",
    "\n",
    "    \n",
    "def write_csv(storage_list, storage_values):\n",
    "    file_name = \"sec_xbrl_scrape_content.csv\"\n",
    "    with open(file_name, mode=\"w\", newline=\"\", encoding=\"utf-8\") as sec_file:\n",
    "        writer = csv.writer(sec_file)\n",
    "        writer.writerow([\"FILE\", \"LABEL\", \"VALUE\"])\n",
    "        for entry in storage_list:\n",
    "            ns_label = entry[0]\n",
    "            data = entry[1]\n",
    "            if isinstance(data, dict):\n",
    "                for k, v in data.items():\n",
    "                    writer.writerow([ns_label, k, v])\n",
    "            else:\n",
    "                writer.writerow([ns_label, str(data)])\n",
    "\n",
    "    file_name = \"sec_xbrl_scrape_values.csv\"\n",
    "    with open(file_name, mode=\"w\", newline=\"\", encoding=\"utf-8\") as sec_file:\n",
    "        writer = csv.writer(sec_file)\n",
    "        writer.writerow([\"ID\", \"CATEGORY\", \"LABEL\", \"VALUE\"])\n",
    "        for id_key, id_val in storage_values.items():\n",
    "            if isinstance(id_val, dict):\n",
    "                for cat_key, cat_val in id_val.items():\n",
    "                    if isinstance(cat_val, dict):\n",
    "                        for label, value in cat_val.items():\n",
    "                            writer.writerow([id_key, cat_key, label, value])\n",
    "                    else:\n",
    "                        # cat_val is a scalar or list\n",
    "                        writer.writerow([id_key, cat_key, \"\", cat_val])\n",
    "            else:\n",
    "                writer.writerow([id_key, \"\", \"\", id_val])\n",
    "\n",
    "    # Facts CSV\n",
    "    facts = storage_values.get(\"_facts_list\", [])\n",
    "    if facts:\n",
    "        with open(\"sec_xbrl_facts.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as ff:\n",
    "            w = csv.writer(ff)\n",
    "            w.writerow([\"tag_prefix\", \"tag_local\", \"gaap_candidate\", \"contextRef\", \"unitRef\", \"decimals\", \"value_raw\"])\n",
    "            for f in facts:\n",
    "                w.writerow([\n",
    "                    f.get(\"tag_prefix\"),\n",
    "                    f.get(\"tag_local\"),\n",
    "                    f.get(\"gaap_id_candidate\"),\n",
    "                    f.get(\"contextRef\"),\n",
    "                    f.get(\"unitRef\"),\n",
    "                    f.get(\"decimals\"),\n",
    "                    f.get(\"value_raw\"),\n",
    "        \n",
    "                ])\n",
    "\n",
    "    contexts = storage_values.get(\"_contexts\", {})\n",
    "    if contexts:\n",
    "        with open(\"sec_xbrl_contexts.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as cf:\n",
    "            w = csv.writer(cf)\n",
    "            w.writerow([\"contextRef\", \"entity_identifier\", \"period_start\", \"period_end\", \"instant\"])\n",
    "            for k, v in contexts.items():\n",
    "                w.writerow([k, v.get(\"entity_identifier\"), v.get(\"period_start\"), v.get(\"period_end\"), v.get(\"instant\")])\n",
    "\n",
    "\n",
    "# main workflow\n",
    "def main():\n",
    "    \n",
    "    # call parser\n",
    "    args = parse_args()\n",
    "\n",
    "    cik = get_cik_from_ticker(args.ticker)\n",
    "\n",
    "    # populate fmap\n",
    "    fmap = {}\n",
    "    \n",
    "    fmap = get_url(cik, args.ticker, args.date)\n",
    "\n",
    "    sec_directory = pathlib.Path.cwd().joinpath(\"folder_to_store_xml_docs\")\n",
    "    sec_directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    file_htm = None\n",
    "    file_cal = None\n",
    "    file_def = None\n",
    "    file_lab = None\n",
    "    file_pre = None\n",
    "    \n",
    "    for fname, url in fmap.items():\n",
    "        fpath = sec_directory / fname\n",
    "        if fname.endswith(\"_htm.xml\"):\n",
    "            file_htm = sec_directory / fname\n",
    "        if fname.endswith(\"_cal.xml\"):\n",
    "            file_cal = sec_directory / fname\n",
    "        if fname.endswith(\"_def.xml\"):\n",
    "            file_def = sec_directory / fname\n",
    "        if fname.endswith(\"_lab.xml\"):\n",
    "            file_lab = sec_directory / fname\n",
    "        if fname.endswith(\"_pre.xml\"):\n",
    "            file_pre = sec_directory / fname\n",
    "        if not fpath.exists():\n",
    "            try:\n",
    "                response = requests.get(url, headers=HEADERS_URL)\n",
    "                time.sleep(0.2) \n",
    "                #response = requests.get(f\"{BASE}/{tail}\", headers = HEADERS_URL)\n",
    "                response.raise_for_status()\n",
    "                fpath.write_bytes(response.content)\n",
    "                print(f\"Downloaded: {fname}\")\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"Error downloading {fname}: {e}\")\n",
    "        else:\n",
    "            print(f\"Already exists: {fname}\")\n",
    "\n",
    "        \n",
    "    # create constructor for named tuple object type\n",
    "    FilingTuple = collections.namedtuple(\"FilingTuple\", [\"file_path\", \"namespace_element\", \"namespace_label\"])\n",
    "    \n",
    "    files_list = [\n",
    "        FilingTuple(file_cal, '{http://www.xbrl.org/2003/linkbase}calculationLink', 'calculation'),\n",
    "        FilingTuple(file_def, '{http://www.xbrl.org/2003/linkbase}definitionLink', 'definition'),\n",
    "        FilingTuple(file_lab, '{http://www.xbrl.org/2003/linkbase}labelLink', 'label'),\n",
    "        FilingTuple(file_pre, '{http://www.xbrl.org/2003/linkbase}presentationLink', 'presentation')\n",
    "    ]\n",
    "    \n",
    "    # label categories\n",
    "    # labelArc points to next element you want\n",
    "\n",
    "    storage_list, storage_values, storage_gaap = parse_linkbases(files_list, parse)\n",
    "    parse_instance_doc(file_htm, storage_values, storage_list, storage_gaap)\n",
    "    write_csv(storage_list, storage_values)\n",
    "    \n",
    "    unmapped = storage_values.get(\"_unmapped_facts\", [])\n",
    "    if unmapped:\n",
    "        print(f\"NOTE: {len(unmapped)} unmapped facts found. Inspect 'sec_xbrl_facts.csv' and storage_values['_unmapped_facts']\")\n",
    "\n",
    "    print(\"Written to csv successfully\")\n",
    "    \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()         \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
