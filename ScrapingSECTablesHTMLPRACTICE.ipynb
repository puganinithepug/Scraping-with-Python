{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10a76755-349a-4ff5-9778-261271245b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Provide company ticker: AAPL\n",
      "Provide year 2021\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found CIK 0000320193 for ticker AAPL\n",
      "https://data.sec.gov/submissions/CIK0000320193.json\n",
      "Found 10-K filing for AAPL 2021: https://www.sec.gov/Archives/edgar/data/320193/000032019324000123/aapl-20240928.htm\n",
      "https://www.sec.gov/Archives/edgar/data/320193/000032019324000123/aapl-20240928.htm\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Type of data to extract, ie tax provisions, gross margins, income income\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16 matching tables.\n",
      "Table 0 saved as sheet in tables_output/financial_statements.xlsx\n",
      "Table 2 saved as sheet in tables_output/financial_statements.xlsx\n",
      "Table 3 saved as sheet in tables_output/financial_statements.xlsx\n",
      "Table 4 saved as sheet in tables_output/financial_statements.xlsx\n",
      "Table 5 saved as sheet in tables_output/financial_statements.xlsx\n",
      "Table 6 saved as sheet in tables_output/financial_statements.xlsx\n",
      "Table 7 saved as sheet in tables_output/financial_statements.xlsx\n",
      "Table 8 saved as sheet in tables_output/financial_statements.xlsx\n",
      "Table 9 saved as sheet in tables_output/financial_statements.xlsx\n",
      "Table 10 saved as sheet in tables_output/financial_statements.xlsx\n",
      "Table 12 saved as sheet in tables_output/financial_statements.xlsx\n",
      "Table 13 saved as sheet in tables_output/financial_statements.xlsx\n",
      "All valid tables saved to tables_output/financial_statements.xlsx\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup, XMLParsedAsHTMLWarning\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=XMLParsedAsHTMLWarning)\n",
    "\n",
    "BASE = \"https://data.sec.gov/submissions/\"\n",
    "\n",
    "#reusable header for everywhere so website allows you to pass without seeming a bot\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"DataResearchBot/1.0 (contact: some@example.com)\",\n",
    "    \"Accept-Encoding\": \"gzip, deflate\",\n",
    "    \"Host\": \"www.sec.gov\"\n",
    "}\n",
    "\n",
    "# first trying with hardcoded URL of Google's 10-K\n",
    "# Now we want to see if it can work with any other SEC URL\n",
    "# It should, so then we can allow the user to choose it, to input it\n",
    "# Reusable for any SEC, for any company, for available years in the SEC archives\n",
    "# we make it simple for the user, asking them for a company name and year then finding the corresponding URL\n",
    "# proceed as before using that URL to pull consolidated financial statements\n",
    "\n",
    "# query user\n",
    "tkr = input(\"Provide company ticker:\").strip()\n",
    "yr = input(\"Provide year\").strip()\n",
    "\n",
    "# use data to find URL\n",
    "\n",
    "# go from company ticker to CIK\n",
    "# CIK = central index key\n",
    "def get_cik_from_ticker(ticker: str) -> str:\n",
    "    url = \"https://www.sec.gov/files/company_tickers.json\"\n",
    "    data = requests.get(url, headers=HEADERS).json()\n",
    "    for entry in data.values():\n",
    "        if entry['ticker'].lower() == ticker.lower():\n",
    "            cik = str(entry['cik_str']).zfill(10)\n",
    "            print(f\"Found CIK {cik} for ticker {ticker}\")\n",
    "            return cik\n",
    "    raise ValueError(f\"Ticker {ticker} not found in SEC database\")\n",
    "\n",
    "#Helper to get 10-K filing URL for a year\n",
    "def get_10k_url(ticker: str, year: str) -> str:\n",
    "    cik = get_cik_from_ticker(ticker)\n",
    "    json_link = f\"{BASE}CIK{cik}.json\"\n",
    "    print(json_link)\n",
    "    headers = {\n",
    "    \"User-Agent\": \"MyResearchBot/1.0 (contact: myemail@example.com)\",\n",
    "    \"Accept-Encoding\": \"gzip, deflate\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "    \"Connection\": \"keep-alive\"}\n",
    "    resp = requests.get(json_link, headers=headers)\n",
    "    resp.raise_for_status()\n",
    "    if \"application/json\" not in resp.headers.get(\"Content-Type\", \"\"):\n",
    "        print(\"Unexpected content:\", resp.text[:200])\n",
    "        raise RuntimeError(f\"Did not receive JSON from SEC for {url}\")\n",
    "    data = resp.json()\n",
    "    forms = data['filings']['recent']\n",
    "    \n",
    "    filings = data.get(\"filings\", {}).get(\"recent\", {})\n",
    "    forms = filings.get(\"form\", [])\n",
    "    dates = filings.get(\"filingDate\", [])\n",
    "    accessions = filings.get(\"accessionNumber\", [])\n",
    "    documents = filings.get(\"primaryDocument\", [])\n",
    "    \n",
    "    for form, date, acc, doc in zip(forms, dates, accessions, documents):\n",
    "        # Only check form type\n",
    "        if form.startswith(\"10-K\"):\n",
    "            acc_no_dashes = acc.replace(\"-\", \"\")\n",
    "            filing_url = (\n",
    "                f\"https://www.sec.gov/Archives/edgar/data/\"\n",
    "                f\"{int(cik)}/{acc_no_dashes}/{doc}\"\n",
    "            )\n",
    "            print(f\"Found 10-K filing for {ticker} {year}: {filing_url}\")\n",
    "            return filing_url  # return immediately\n",
    "\n",
    "    # if no 10-K found at all\n",
    "    raise ValueError(f\"No 10-K filing found for {ticker} in {year}\")\n",
    "\n",
    "url = get_10k_url(tkr, yr)\n",
    "print(url)\n",
    "\n",
    "# 1: Load & parse HTML\n",
    "resp = requests.get(url, headers=HEADERS)\n",
    "resp.raise_for_status()\n",
    "soup = BeautifulSoup(resp.text, \"lxml\")\n",
    "\n",
    "# 2: Find consolidated statement tables\n",
    "# query user for the daesired data for extraction into xlsx\n",
    "desired_data = input(\"Type of data to extract, ie tax provisions, gross margins, income\")\n",
    "all_tables = soup.find_all(\"table\")\n",
    "target_tables = []\n",
    "for table in all_tables:\n",
    "    text = table.get_text(\" \", strip=True).lower()\n",
    "    if desired_data in text:\n",
    "        target_tables.append(table)\n",
    "\n",
    "print(f\"Found {len(target_tables)} matching tables.\")\n",
    "\n",
    "# Helper to clean rows\n",
    "def extract_table(table):\n",
    "    rows = []\n",
    "    for row in table.find_all(\"tr\"):\n",
    "        cells = []\n",
    "        for cell in row.find_all([\"td\", \"th\"]):\n",
    "            txt = cell.get_text(\" \", strip=True)\n",
    "            if txt.endswith(\",\"): txt=txt.rstrip(\",\")\n",
    "            txt = txt.replace(u'\\xa0', ' ')       # fix &nbsp;\n",
    "            txt = unicodedata.normalize(\"NFKC\", txt)\n",
    "            if txt not in [\"$\", \"%\"]:  # skip lone symbols\n",
    "                # Convert (90) into -90\n",
    "                if txt.startswith(\"(\") and txt.endswith(\")\"):\n",
    "                    inner = txt[1:-1]  # strip parentheses\n",
    "                    txt = \"-\" + inner\n",
    "                cells.append(txt)\n",
    "        if cells:\n",
    "            # collapse duplicates: [\"Revenue\",\"Revenue\",\"90,272\"] -> [\"Revenue\",\"90,272\"]\n",
    "            cleaned = []\n",
    "            for c in cells:\n",
    "                if not cleaned or c != cleaned[-1]:\n",
    "                    cleaned.append(c)\n",
    "            rows.append(cleaned)\n",
    "    first_cell = rows[0][0].strip().lower()\n",
    "    # Normalize all text to lowercase for comparison\n",
    "    first_row = [c.strip().lower() for c in rows[0]]\n",
    "    first_col = [r[0].strip().lower() for r in rows if r]\n",
    "\n",
    "    # Check if any cell in first row OR first column contains \"page\" or \"index\"\n",
    "    if any(\"page\" in c or \"index\" in c for c in first_row + first_col):\n",
    "        return None\n",
    "    \n",
    "    return pd.DataFrame(rows) if (rows and len(rows) > 4 and max(len(r) for r in rows) > 2) else None\n",
    "\n",
    "\n",
    "\n",
    "# Step 3: Take all target table and extract\n",
    "if not target_tables:\n",
    "    print(\"No matching tables found.\")\n",
    "# now, need to recursively extract and place each table found on a new csv\n",
    "# make new worksheets in one workbook, one per extracted table \n",
    "else:\n",
    "    # folder for files\n",
    "    output_dir = \"tables_output\"\n",
    "    # define folder to store files\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    output_file = os.path.join(output_dir, \"financial_statements.xlsx\")\n",
    "\n",
    "    #ExcelWriter saves all tables into one workbook\n",
    "\n",
    "    with pd.ExcelWriter(output_file, engine=\"openpyxl\") as writer:\n",
    "    \n",
    "        for i, table_tag in enumerate(target_tables):\n",
    "            raw_df = extract_table(table_tag)\n",
    "    \n",
    "            if raw_df is None:\n",
    "                continue\n",
    "        \n",
    "            # Reload and clean\n",
    "            df = raw_df.fillna(\"\")\n",
    "            df = df.drop_duplicates()                      # Remove duplicate rows\n",
    "            df = df.loc[:, ~df.T.duplicated()]             # Remove duplicate columns\n",
    "            \n",
    "            df.to_excel(writer, sheet_name=f\"table_{i}\"[:31], index=False)\n",
    "      \n",
    "            print(f\"Table {i} saved as sheet in {output_file}\")\n",
    "\n",
    "    print(f\"All valid tables saved to {output_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
