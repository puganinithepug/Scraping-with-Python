{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "8e5f53ee-92d0-41d4-b308-5ea56f789838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Company ticker pltr\n",
      "Date 2025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found CIK 0001321655 for ticker pltr\n",
      "https://data.sec.gov/submissions/CIK0001321655.json\n",
      "https://www.sec.gov/Archives/edgar/data/1321655/000132165525000110/index.json\n",
      "https://www.sec.gov/Archives/edgar/data/1321655/000132165525000109/index.json\n",
      "https://www.sec.gov/Archives/edgar/data/1321655/000195004725005441/index.json\n",
      "https://www.sec.gov/Archives/edgar/data/1321655/000132165525000106/index.json\n",
      "Found filing with required XMLs at: https://www.sec.gov/Archives/edgar/data/1321655/000132165525000106/index.json\n",
      "{'pltr-20250630_cal.xml': 'https://www.sec.gov/Archives/edgar/data/1321655/000132165525000106/pltr-20250630_cal.xml', 'pltr-20250630_def.xml': 'https://www.sec.gov/Archives/edgar/data/1321655/000132165525000106/pltr-20250630_def.xml', 'pltr-20250630_htm.xml': 'https://www.sec.gov/Archives/edgar/data/1321655/000132165525000106/pltr-20250630_htm.xml', 'pltr-20250630_lab.xml': 'https://www.sec.gov/Archives/edgar/data/1321655/000132165525000106/pltr-20250630_lab.xml'}\n",
      "Already exists: pltr-20250630_cal.xml\n",
      "Already exists: pltr-20250630_def.xml\n",
      "Already exists: pltr-20250630_htm.xml\n",
      "Already exists: pltr-20250630_lab.xml\n",
      "Written to csv successfully\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import pprint\n",
    "import pathlib\n",
    "import collections\n",
    "# API for parsing and creating XML data\n",
    "import xml.etree.ElementTree as ET\n",
    "import lxml.etree as ETL\n",
    "import requests\n",
    "import argparse\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# map ticker to CIK\n",
    "TICKER_JSON = \"https://www.sec.gov/files/company_tickers.json\"\n",
    "\n",
    "# get accession number from cik\n",
    "SUB_URL = \"https://data.sec.gov/submissions/\"\n",
    "\n",
    "BASE = \"https://www.sec.gov/Archives/edgar/data/\"\n",
    "\n",
    "#reusable header for sec.gov, complying with standards so website allows you to pass without seeming a bot\n",
    "HEADERS_URL = {\n",
    "    \"User-Agent\": \"MyResearchBot/1.0 (contact: myemail@example.com)\",\n",
    "    \"Accept-Encoding\": \"gzip, deflate\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "    \"Connection\": \"keep-alive\"}\n",
    "\n",
    "# keys used to parse the document for desired data\n",
    "parse = ['label', 'labelLink', 'labelArc', 'loc', 'definitionLink', 'definitionArc', 'calculationArc']\n",
    "\n",
    "# Max CIK matches count\n",
    "MAX_COUNT = 100\n",
    "\n",
    "# parser\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Fetch SEC XMLs\"\n",
    "    )\n",
    "    parser.add_argument(\"--ticker\", help=\"Company ticker\", default=None)\n",
    "    parser.add_argument(\"--date\", help=\"Date\", default=None)\n",
    "    # Accept unknown args\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    # Prompt user interactvely if missing arguments\n",
    "    if not args.ticker:\n",
    "        args.ticker = input(\"Company ticker\")\n",
    "        args.date = input(\"Date\")\n",
    "    return args\n",
    "\n",
    "# go from company ticker to CIK\n",
    "def get_cik_from_ticker(ticker: str) -> str:\n",
    "    url = TICKER_JSON\n",
    "    data = requests.get(url, headers=HEADERS_URL).json()\n",
    "    for entry in data.values():\n",
    "        # get json mapping from SEC and search of company ticker\n",
    "        if entry['ticker'].lower() == ticker.lower():\n",
    "            # fetch CIK\n",
    "            cik = str(entry['cik_str']).zfill(10)\n",
    "            print(f\"Found CIK {cik} for ticker {ticker}\")\n",
    "            return cik\n",
    "    raise ValueError(f\"Ticker {ticker} not found in SEC database\")\n",
    "\n",
    "# creates a mapping for xml documents and their urls\n",
    "def get_url(cik, ticker, year: str) -> str:\n",
    "    # cik = get_cik_from_ticker(ticker)\n",
    "    json_link = f\"{SUB_URL}CIK{cik}.json\"\n",
    "    print(json_link)\n",
    "    headers = HEADERS_URL\n",
    "    resp = requests.get(json_link, headers=headers)\n",
    "    resp.raise_for_status()\n",
    "    if \"application/json\" not in resp.headers.get(\"Content-Type\", \"\"):\n",
    "        print(\"Unexpected content:\", resp.text[:200])\n",
    "        raise RuntimeError(f\"Did not receive JSON from SEC for {url}\")\n",
    "    data = resp.json()\n",
    "    forms = data['filings']['recent']\n",
    "    \n",
    "    filings = data.get(\"filings\", {}).get(\"recent\", {})\n",
    "    forms = filings.get(\"form\", [])\n",
    "    dates = filings.get(\"filingDate\", [])\n",
    "    accessions = filings.get(\"accessionNumber\", [])\n",
    "    documents = filings.get(\"primaryDocument\", [])\n",
    "\n",
    "    filings_data = list(zip(forms, dates, accessions, documents))\n",
    "    # Sort by date descending\n",
    "    filings_data.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    for form, date, acc, doc in filings_data:\n",
    "        acc_no_dashes = acc.replace(\"-\", \"\")\n",
    "        # Build base URL\n",
    "        base_url = f\"{BASE}{int(cik)}/{acc_no_dashes}/\"\n",
    "        \n",
    "        # Fetch filing's index.json (listing all files)\n",
    "        index_url = f\"{base_url}index.json\"\n",
    "        print(index_url)\n",
    "        try:\n",
    "            r = requests.get(index_url, headers=headers)\n",
    "            r.raise_for_status()\n",
    "            idx_data = r.json()\n",
    "\n",
    "            # Collect the relevant XML files found here\n",
    "            found_files = {}\n",
    "            for file_entry in idx_data.get(\"directory\", {}).get(\"item\", []):\n",
    "                name = file_entry.get(\"name\", \"\")\n",
    "                # Check if file is one of the XML types you want\n",
    "                if any(suffix in name for suffix in [\"_def.xml\", \"_htm.xml\", \"_lab.xml\", \"_cal.xml\"]):\n",
    "                    # Use keys without extensions for clarity or keep names as is\n",
    "                    found_files[name] = base_url + name\n",
    "\n",
    "            # If found at least the instance document (_htm.xml), consider success\n",
    "            if any(\"_htm.xml\" in fname for fname in found_files):\n",
    "                print(f\"Found filing with required XMLs at: {index_url}\")\n",
    "                print(found_files)\n",
    "                # Scans the directory items for the XML files you want (_def.xml, _htm.xml, _lab.xml, _cal.xml\n",
    "                # returns dict with found required elements of fmap (some may be missing)\n",
    "                return found_files  # Return dictionary of files with full URLs\n",
    "\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Error fetching {index_url}: {e}\")\n",
    "            continue  # Try next filing\n",
    "\n",
    "    # If none found\n",
    "    raise ValueError(f\"No filing found with required XML files for CIK {cik} and year {year}\")\n",
    "\n",
    "#can loop through each file\n",
    "# P3\n",
    "# del if breaks\n",
    "def parse_linkbases(files_list, parse):\n",
    "\n",
    "    # list\n",
    "    storage_list = []\n",
    "    \n",
    "    # dictionary \n",
    "    storage_values= {}\n",
    "    \n",
    "    # another dictionary\n",
    "    storage_gaap = {}\n",
    "\n",
    "    dict_storage = {}\n",
    "\n",
    "    \n",
    "    for file in files_list:\n",
    "    \n",
    "        #print(file)\n",
    "        # that returns first item in file tuple, which is a list\n",
    "        # we want to access file so we do files_list[0]\n",
    "        # i.e file_cal for instance\n",
    "        if file[0] is None:\n",
    "            print(f\"Warning: File for {file.namespace_label} not found or missing, skipping.\")\n",
    "            continue\n",
    "       \n",
    "        # parse file\n",
    "        tree = ET.parse(file.file_path)\n",
    "        # create element tree\n",
    "        # print(tree)\n",
    "    \n",
    "        # grab all namespace_elements in tree\n",
    "        elements = tree.findall(file.namespace_element)\n",
    "        # will return all elements that match this: \n",
    "        # http://www.xbrl.org/2003/linkbase)calculationLink namespace\n",
    "        #print(elements)\n",
    "    \n",
    "        # loop through each element\n",
    "        # loop through child elements\n",
    "        # P4\n",
    "        for element in elements:\n",
    "            # create iterator\n",
    "            # loop through child element of each element\n",
    "            for child_element in element.iter():\n",
    "    \n",
    "                #print(child_element)\n",
    "                # get elements and their children from document\n",
    "                # next is getting attributes of elements\n",
    "                element_split_label = child_element.tag.split(\"}\")\n",
    "                # print(element_split_label)\n",
    "                # want to remove the redundant prefix on label:\n",
    "                # {http://www.xbrl.org/2003/linkbase}\n",
    "                # get parts of label\n",
    "                namespace = element_split_label[0]\n",
    "                label = element_split_label[1]\n",
    "                # is this label we want?\n",
    "                # wanted labels in parse\n",
    "                if label in parse:\n",
    "                    element_type_label = file.namespace_label + \"_\" + label\n",
    "                    #print(element_type_label)\n",
    "    \n",
    "                    # define dictionary\n",
    "                    dict_storage = {}\n",
    "                    dict_storage[\"item_type\"] = element_type_label\n",
    "    \n",
    "                    # get attribute keys\n",
    "                    cal_keys = child_element.keys()\n",
    "                    # print(cal_keys)\n",
    "    \n",
    "                    for key in cal_keys:\n",
    "                        if \"}\" in key:\n",
    "                            new_key = key.split(\"}\")[1]\n",
    "                            dict_storage[new_key] = child_element.attrib[key]\n",
    "                        else:\n",
    "                            dict_storage[key] = child_element.attrib[key]\n",
    "                    #print(dict_storage)\n",
    "    \n",
    "                    # choosing master key to be the label document\n",
    "                    # could choose anything else - experimental\n",
    "                    if element_type_label == \"label_label\":\n",
    "                        key_store = dict_storage[\"label\"]\n",
    "    \n",
    "                        # create master key\n",
    "                        master_key = key_store.replace(\"lab_\", \"\")\n",
    "    \n",
    "                        # split master key\n",
    "                        label_split = master_key.split(\"_\")\n",
    "    \n",
    "                        #a\n",
    "                        # create gaap id\n",
    "                        gaap_id =  label_split[0] + \";\" + label_split[1]\n",
    "                        #print(label_split)\n",
    "                        # there are duplicates\n",
    "                        # thats why we put it in a dicionary - unique key to value\n",
    "                        # dict for xml files\n",
    "                        storage_values[master_key] = {}\n",
    "                        # dictionary storage values is created with the master key\n",
    "                      \n",
    "                        storage_values[master_key][\"label_id\"] = key_store\n",
    "                        storage_values[master_key][\"location_id\"] = key_store.replace(\"lab_\", \"loc_\")\n",
    "                        storage_values[master_key][\"us_gaap_id\"] = gaap_id\n",
    "                        storage_values[master_key][\"us_gaap_values\"] = None\n",
    "                        storage_values[master_key][element_type_label] = dict_storage\n",
    "                        #b is a subdictiory of a\n",
    "                        # dict for only values related to GAAP\n",
    "                        storage_gaap[gaap_id] = {}\n",
    "                        storage_gaap[gaap_id][\"id\"] = gaap_id\n",
    "                        storage_gaap[gaap_id][\"master_id\"] = master_key\n",
    "                        # a and b should be merged\n",
    "                        # master keys created in big dictiory\n",
    "                        # master key associated with smaller dictiory for GAAP stuff exclusively, organized as in b\n",
    "            # add to dict\n",
    "            storage_list.append([file.namespace_label, dict_storage])\n",
    "            # parsing the html file with nonNumeric and nonFractional stuff\n",
    "            # parse 10Q file\n",
    "            # load file_htm\n",
    "    # del if breaks\n",
    "    return storage_list, storage_values, storage_gaap\n",
    "\n",
    "def parse_instance_doc(file_htm, storage_values, storage_list, storage_gaap):        \n",
    "        tree = ET.parse(file_htm)\n",
    "        # Process nonNumeric elements\n",
    "        for element in tree.iter():\n",
    "            #print(element.attrib)\n",
    "            if \"nonNumeric\" in element.tag or \"nonFractional\" in element.tag:\n",
    "                # get attribute name and master id\n",
    "                attr_name = element.attrib.get(\"name\")\n",
    "                if not attr_name or attr_name not in storage_gaap: \n",
    "                    continue\n",
    "                storage_gaap[attr_name][\"context_ref\"] = element.attrib[\"contextRef\"]\n",
    "                storage_gaap[attr_name][\"context_id\"] = element.attrib[\"id\"]\n",
    "                storage_gaap[attr_name][\"continued_at\"] = element.attrib.get(\"continuedAt\", \"null\")\n",
    "                storage_gaap[attr_name][\"escape\"] = element.attrib.get(\"escape\", \"null\")\n",
    "                storage_gaap[attr_name][\"format\"] = element.attrib.get(\"format\", \"null\")\n",
    "                storage_gaap[attr_name][\"unit_ref\"] = element.attrib.get(\"unitRef\", \"null\")\n",
    "                storage_gaap[attr_name][\"decimals\"] = element.attrib.get(\"decimals\", \"null\")\n",
    "                storage_gaap[attr_name][\"scale\"] = element.attrib.get(\"scale\", \"null\")\n",
    "                storage_gaap[attr_name][\"format\"] = element.attrib.get(\"format\", \"null\")\n",
    "                storage_gaap[attr_name][\"value\"] = element.text.strip() if element.text else \"null\"\n",
    "        \n",
    "                if gaap_id in storage_values:\n",
    "                    storage_values[gaap_id][\"us_gaap_value\"] = storage_gaap[attr_name]     \n",
    "\n",
    "\n",
    "def write_csv(storage_list, storage_values):\n",
    "    # create csv\n",
    "    file_name = \"sec_xbrl_scrape_content.csv\"\n",
    "\n",
    "    with open(file_name, mode = \"w\", newline = \"\") as sec_file:\n",
    "        #create writer\n",
    "        writer = csv.writer(sec_file)\n",
    "        # write the header\n",
    "        # pass to the row writer the list of things to go into the header\n",
    "        writer.writerow([\"FILE\", \"LABEL\", \"VALUE\"])\n",
    "        # dump dict into csv\n",
    "        for dict_cont in storage_list:\n",
    "            # write row by row the things stored inside the storage list\n",
    "            # the first is the namespace label\n",
    "            # the second item is the actual dict\n",
    "            for item in dict_cont[1].items():\n",
    "                # second item is list of lists\n",
    "                # grab items per each item\n",
    "                writer.writerow([dict_cont[0]] + list(item))\n",
    "            \n",
    "    # create csv\n",
    "    file_name = \"sec_xbrl_scrape_values.csv\"\n",
    "\n",
    "    with open(file_name, mode = \"w\", newline = \"\") as sec_file:\n",
    "        writer = csv.writer(sec_file)\n",
    "        writer.writerow([\"ID\", \"CATEGORY\", \"LABEL\", \"VALUE\"])\n",
    "        for storage1 in storage_values:\n",
    "            # storage1 are keys to the values extracted from the second level dict\n",
    "            # the .items() call enumerates values in dict\n",
    "            for storage2 in storage_values[storage1].items():\n",
    "                # extract by key the value\n",
    "                # the value might be another dict because elements can have child elements\n",
    "                if isinstance(storage2[1], dict): # check if it is\n",
    "                    for storage3 in storage2[1].items():\n",
    "                        # write to csv\n",
    "                        writer.writerow([storage1] + [storage2[0]] + list(storage3))\n",
    "                else:\n",
    "                    if storage2[1] != None:\n",
    "                        #write to csv, if storage2 is not a dictionry (we dont go to storage3)\n",
    "                        writer.writerow([storage1] + [storage2] + [\"None\"])\n",
    "# main workflow\n",
    "def main():\n",
    "    \n",
    "    # call parser\n",
    "    args = parse_args()\n",
    "\n",
    "    # results are args.ticker and args.date\n",
    "    # check if date is present\n",
    "    \n",
    "    # if user doesnt provide date, we automate to grab most recent\n",
    "    # if user does provide date then we search for that\n",
    "    \n",
    "    # get cik\n",
    "    #cik = get_cik_from_ticker(args.ticker)\n",
    "\n",
    "   # time.sleep(0.5)  # Half a second between SEC requests\n",
    "    \n",
    "    # get accession number\n",
    "    #accession_num = get_accession_for_date(cik, args.date)\n",
    "\n",
    "    #time.sleep(0.5)  # Half a second between SEC requests\n",
    "\n",
    "    # build url\n",
    "    #base_url = get_base_url(cik, accession_num)\n",
    "\n",
    "    #time.sleep(0.5)  # Half a second between SEC requests\n",
    "\n",
    "    # here we go look for xml files\n",
    "    # either for specified date or for most recent\n",
    "    # file for htm.xml stored in htm\n",
    "    # file for cal.xml stored in cal\n",
    "    # file for def.xml stored in defi\n",
    "\n",
    "    cik = get_cik_from_ticker(args.ticker)\n",
    "\n",
    "    # populate fmap\n",
    "    fmap = {}\n",
    "    \n",
    "    fmap = get_url(cik, args.ticker, args.date)\n",
    "\n",
    "    sec_directory = pathlib.Path.cwd().joinpath(\"folder_to_store_xml_docs\")\n",
    "    sec_directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    file_htm = None\n",
    "    file_cal = None\n",
    "    file_def = None\n",
    "    file_lab = None\n",
    "    \n",
    "    for fname, url in fmap.items():\n",
    "        fpath = sec_directory / fname\n",
    "        if fname.endswith(\"_htm.xml\"):\n",
    "            file_htm = sec_directory / fname\n",
    "        if fname.endswith(\"_cal.xml\"):\n",
    "            file_cal = sec_directory / fname\n",
    "        if fname.endswith(\"_def.xml\"):\n",
    "            file_def = sec_directory / fname\n",
    "        if fname.endswith(\"_lab.xml\"):\n",
    "            file_lab = sec_directory / fname\n",
    "        if not fpath.exists():\n",
    "            try:\n",
    "                response = requests.get(url, headers=HEADERS_URL)\n",
    "                time.sleep(0.2) \n",
    "                #response = requests.get(f\"{BASE}/{tail}\", headers = HEADERS_URL)\n",
    "                response.raise_for_status()\n",
    "                fpath.write_bytes(response.content)\n",
    "                print(f\"Downloaded: {fname}\")\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"Error downloading {fname}: {e}\")\n",
    "        else:\n",
    "            print(f\"Already exists: {fname}\")\n",
    "\n",
    "        \n",
    "    # create constructor for named tuple object type\n",
    "    FilingTuple = collections.namedtuple(\"FilingTuple\", [\"file_path\", \"namespace_element\", \"namespace_label\"])\n",
    "    \n",
    "    files_list = [\n",
    "        FilingTuple(file_cal, '{http://www.xbrl.org/2003/linkbase}calculationLink', 'calculation'),\n",
    "        FilingTuple(file_def, '{http://www.xbrl.org/2003/linkbase}definitionLink', 'definition'),\n",
    "        FilingTuple(file_lab, '{http://www.xbrl.org/2003/linkbase}labelLink', 'label')\n",
    "    ]\n",
    "    \n",
    "    # label categories\n",
    "    # labelArc points to next element you want\n",
    "\n",
    "    storage_list, storage_values, storage_gaap = parse_linkbases(files_list, parse)\n",
    "    parse_instance_doc(file_htm, storage_values, storage_list, storage_gaap)\n",
    "    write_csv(storage_list, storage_values)\n",
    "    print(\"Written to csv successfully\")\n",
    "    \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518b6303-b85a-49fc-b98c-2ca2fc53892d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e4da04-60fd-41a2-9e70-043bd3ecb216",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d73d24-6ef0-4a00-a4cc-067c345afdc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
